{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting: TSLA Stock\n",
    "\n",
    "This notebook compares classical (ARIMA/SARIMA) and deep learning (LSTM) models for forecasting Tesla stock prices. Code is modular and OOP-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOP-based Data Preparation for Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSLADataHandler:\n",
    "    def __init__(self, start='2018-01-01', end=None):\n",
    "        self.ticker = 'TSLA'\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.data = None\n",
    "\n",
    "    def load(self):\n",
    "        self.data = yf.download(self.ticker, start=self.start, end=self.end, auto_adjust=True)['Close']\n",
    "        return self.data\n",
    "\n",
    "    def train_test_split(self, test_years=2):\n",
    "        split_idx = int(len(self.data) * (1 - test_years / ((pd.to_datetime(self.data.index[-1]).year + 1) - pd.to_datetime(self.data.index[0]).year)))\n",
    "        train, test = self.data.iloc[:split_idx], self.data.iloc[split_idx:]\n",
    "        return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Split Data (Chronologically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = TSLADataHandler(start='2015-01-01')\n",
    "tsla_close = handler.load()\n",
    "train, test = handler.train_test_split(test_years=2)\n",
    "print(f'Train: {train.index[0]} to {train.index[-1]}')\n",
    "print(f'Test: {test.index[0]} to {test.index[-1]}')\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='Test')\n",
    "plt.title('TSLA Closing Price: Train/Test Split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARIMA/SARIMA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use auto_arima for best (p,d,q)\n",
    "arima_model = pm.auto_arima(train, seasonal=False, stepwise=True, suppress_warnings=True, trace=True)\n",
    "print(f'Best ARIMA order: {arima_model.order}')\n",
    "arima_fit = ARIMA(train, order=arima_model.order).fit()\n",
    "arima_forecast = arima_fit.forecast(steps=len(test))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='Test')\n",
    "plt.plot(test.index, arima_forecast, label='ARIMA Forecast')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast vs Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecast(true, pred):\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(true, pred))\n",
    "    mape = np.mean(np.abs((true - pred) / true)) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "arima_mae, arima_rmse, arima_mape = evaluate_forecast(test, arima_forecast)\n",
    "print(f'ARIMA MAE: {arima_mae:.2f}, RMSE: {arima_rmse:.2f}, MAPE: {arima_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Modeling (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster:\n",
    "    def __init__(self, lookback=30):\n",
    "        self.lookback = lookback\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def prepare_data(self, series):\n",
    "        scaled = self.scaler.fit_transform(series.values.reshape(-1,1))\n",
    "        X, y = [], []\n",
    "        for i in range(self.lookback, len(scaled)):\n",
    "            X.append(scaled[i-self.lookback:i, 0])\n",
    "            y.append(scaled[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        return X, y\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, activation='relu', input_shape=(self.lookback, 1)))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y, epochs=20, batch_size=32):\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    def forecast(self, last_sequence, n_steps):\n",
    "        preds = []\n",
    "        seq = last_sequence.copy()\n",
    "        for _ in range(n_steps):\n",
    "            pred = self.model.predict(seq.reshape(1, self.lookback, 1), verbose=0)[0,0]\n",
    "            preds.append(pred)\n",
    "            seq = np.roll(seq, -1)\n",
    "            seq[-1] = pred\n",
    "        return self.scaler.inverse_transform(np.array(preds).reshape(-1,1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMForecaster(lookback=30)\n",
    "X_train, y_train = lstm.prepare_data(train)\n",
    "lstm.build_model()\n",
    "lstm.fit(X_train, y_train, epochs=20, batch_size=32)\n",
    "# Prepare last sequence from train+test for forecasting\n",
    "full_series = pd.concat([train, test])\n",
    "scaled_full = lstm.scaler.fit_transform(full_series.values.reshape(-1,1))\n",
    "last_seq = scaled_full[len(train)-30:len(train)]\n",
    "lstm_forecast = lstm.forecast(last_seq, n_steps=len(test))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='Test')\n",
    "plt.plot(test.index, lstm_forecast, label='LSTM Forecast')\n",
    "plt.legend()\n",
    "plt.title('LSTM Forecast vs Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mae, lstm_rmse, lstm_mape = evaluate_forecast(test, lstm_forecast)\n",
    "print(f'LSTM MAE: {lstm_mae:.2f}, RMSE: {lstm_rmse:.2f}, MAPE: {lstm_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison & Discussion\n",
    "\n",
    "| Model  | MAE  | RMSE | MAPE (%) |\n",
    "|--------|------|------|----------|\n",
    "| ARIMA  | {arima_mae:.2f} | {arima_rmse:.2f} | {arima_mape:.2f} |\n",
    "| LSTM   | {lstm_mae:.2f} | {lstm_rmse:.2f} | {lstm_mape:.2f} |\n",
    "\n",
    "- ARIMA is interpretable, fast to train, but may underperform on highly nonlinear series.\n",
    "- LSTM can model nonlinearities and long-term dependencies, but requires more data and tuning.\n",
    "- Choose based on business needs: interpretability vs. predictive power.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
